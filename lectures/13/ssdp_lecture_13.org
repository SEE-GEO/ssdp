#+TITLE: Scientific Software Development with Python
#+SUBTITLE: High performance computing and big data
#+LaTeX_CLASS_OPTIONS: [10pt]
#+AUTHOR: Simon Pfreundschuh
#+OPTIONS: H:2 toc:nil
#+LaTeX_HEADER: \institute{Department of Space, Earth and Environment}
#+LaTeX_HEADER: \setbeamerfont{title}{family=\sffamily, series=\bfseries, size=\LARGE}
#+LATEX_HEADER: \usepackage[style=authoryear]{biblatex}
#+LATEX_HEADER: \usepackage{siunitx}
#+LaTeX_HEADER: \usetheme{chalmers}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{dirtree}
#+LATEX_HEADER: \usemintedstyle{monokai}
#+LATEX_HEADER: \definecolor{light}{HTML}{CCCCCC}
#+LATEX_HEADER: \definecolor{dark}{HTML}{353535}
#+LATEX_HEADER: \definecolor{green}{HTML}{008800}
#+LATEX_HEADER: \definecolor{source_file}{rgb}{0.82, 0.1, 0.26}
#+LATEX_HEADER: \newmintinline[pyil]{Python}{style=default, bgcolor=light}
#+BEAMER_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{Agenda}\tableofcontents[currentsection]\end{frame}}
#+LATEX_HEADER: \newcommand\blfootnote[1]{\begingroup \renewcommand\thefootnote{}\footnote{#1} \addtocounter{footnote}{-1} \endgroup}


* Introduction
** In this lecture
   1. Solve heat equation on a GPU
   2. Solve the heat equation in a smarter way
   3. Distributed computing using IPython Parallel

** Overview
   - Need to go parallel to go faster
  \centering
  \includegraphics[width=0.9\textwidth]{figures/microprocessors}
    \blfootnote{Image source: https://www.karlrupp.net}

* Programming hardware accelerators
** Hardware accelerators
\blfootnote{Image source: nvidia.com}
*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
   - Hardware accelerators are special computer hardware designed to
     speed up specific tasks
   - Most commonly used today: Graphic processing units (GPUs)
   - Originally designed to display 3D graphics
   - Example: Nvidia T4 (Available on Vera@C3SE)
     - More than 2500 cores
     - 320 of which special 'tensor' cores for machine learning

*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:
\centering
\includegraphics[width=0.6\textwidth]{figures/nvidia.png}

** Hardware accelerators
*** Difficulties
    - Usually have their own separate memory (transfer bottleneck)
    - Require special programming techniques to program

      \vspace{0.5cm}
\centering
\includegraphics[width=0.6\textwidth]{figures/hardware_accelerators}

** Hardware accelerators
*** GPU programming with Python
    - Python won't run directly on GPU.
    - GPUs used through some kind of special array or tensor type
    - Wide range of packages that allow almost platform-agnostic[fn:2]
      computing across different hardware
    - Many of them are behind the recent consolidation of applicaitons
      of deep learning in science.

\vspace{0.4cm}
\includegraphics[width=\textwidth]{figures/packages}      

[fn:2] Platform agnostic: Same code can run on CPU, GPU or whatever hardware is available.
      
** GPU programming with CuPy
*** Introducing CuPy
    - CUDA is NVIDIA[fn:3]'s computing and programming platform
    - CuPY provides drop-in replacement for numpy arrays to accelerate
      array operations.
    - Not all numpy operations implemented but this is 
      the easiest way to perform calculations on GPU.

*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.6
    :END:
**** Installation
      #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
      #+BEGIN_SRC text
      pip install cupy
      #+END_SRC 
      - Or depending on your CUDA version:
      #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
      #+BEGIN_SRC text
      pip install cupy-cudaXX
      #+END_SRC 

*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.4
    :END:

\centering
\includegraphics[width=0.8\textwidth]{figures/cupy}      

[fn:3] NVIDIA is essentially the Intel of GPUs

** GPU programming with CuPy
*** Matrix multiplication example
    - Before it can be used on the GPU, data must be transferred
      to the device.
    - This is done by converting the =numpy.array= into a =cupy.array=
    
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    import numpy as np
    import cupy as cp

    n = 2048
    # Create matrix and vector on host.
    matrix = np.random.rand(n, n)
    vector = np.random.rand(n)

    # Transfer matrix and vector to GPU.
    matrix_gpu = cp.asarray(matrix)
    vector_gpu = cp.asarray(vector)
    
    result = np.dot(matrix, vector
    result_gpu = cp.do(matrix_gpu, vector_gpu)
    #+END_SRC 

** GPU programming with CuPy
*** Platform agnostic matrix multiplication
    - Use =cupy.get_array_module= to get module object (=np= or =cp=)
      corresponding to array.
    
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    def matrix_multiplication(matrix, vector):
        xp = cp.get_array_module(matrix)
        return xp.dot(matrix, vector)

    result = matrix_multiplication(matrix, vector)
    result_gpu = matrix_multiplication(matrix_gpu, vector_gpu)
    #+END_SRC 

**** Performance exmample
    - NVIDIA Tesla T4 vs. Intel Xeon (2 cores)
    - Task probably not heavy enough to show full potential of GPUs.
     
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
    #+BEGIN_SRC text
    %timeit matrix_multiplication(matrix, vector)
    >>> 1000 loops, best of 3: 1.51 ms per loop
    %timeit matrix_multiplication(matrix_gpu, vector_gpu)
    >>> 10000 loops, best of 3: 139 Âµs per loop
    #+END_SRC 

** Exercise 1
*** Exercise 1
    - Exercise 1 from notebook
    - Time: 15 minutes

** GPU programming with Python

*** Summary
    - If you have GPU and CUDA installed =cupy= can provide
      an easy way to accelerate your computations.
    - However, using your GPU through Python will be limited
      by the functionality provided by the package you are using.[fn:5]
    - Other packages that can be used to compute on GPUs:
      - Theano, Numba, TensorFlow, PyTorch, ...

[fn:5] But it is much simpler to do.

* The heat equation revisited
** The heat equation revisited
 
*** The heat equation
   \begin{align}\label{eq:heat}
   \frac{\partial u}{\partial t} = \alpha \left (
    \frac{\partial^2 u}{\partial^2 x} + \frac{\partial^2 u}{\partial^2 y}
    \right )
    \end{align}

    - For the simple case of $\alpha = 1$, the heat equation can be solved explicitly.


** The heat equation revisited
    1. Assuming that $u$ can be written as a function of the form
     \begin{align}
     u(t, x, y) = T(t) \cdot X(x) \cdot Y(y)
     \end{align}
    2. The problem can be transformed to a system of coupled /ordinary/ differential equations:
     \begin{align}
     \frac{\partial^2 X}{\partial^2 x} = A \cdot X \\
     \frac{\partial^2 Y}{\partial^2 y} = B \cdot Y \\
     \frac{\partial T}{\partial t} = (A + B) \cdot Y
     \end{align}

** The heat equation revisited

   - From this we find that a general solution of the heat equation is given
     by:

     \begin{align}
     u(t, x, y) = \sum_{m, n} A_{m, n} \ e^{i\frac{2 \pi m}{N}x} \ e^{i\frac{2 \pi n}{N}y} \ e^{- \frac{4\pi^2(n^2 + m^2)}{N^2}t}
     \end{align}

   - We can thus also solve the heat equation as follows:
     1. Use a 2D Fourier transform to calculate the Fourier coefficients  $A_{m, n}(0)$ from the initial
        heat distribution $u(0, x, y)$.

     2. Multiply coefficients $A_{m, n}(0)$ by $e^{-\frac{4 \pi^2(n^2 + m^2)}{N^2}t}$ to
        obtain coefficients $A_{m, n}(t)$
     3. Calculate $u(t, x, y)$ by calculating the inverse Fourier transform of $A_{m, n}(t)$
        
** Exercise 2
   - Exercise 2 from notebook.
   - Time: 20 minutes.

* Distributed computing with IPython Parallel

** Distributed computing
   - A program that executes concurrently across different computers
   - Instances of the program typically do not share memory
   - Special messaging functions required for communication
   - Popular software packages:
     - High performance computing: Message passing interface (MPI)
     - Big data: Hadoop, Dask

\vspace{0.5cm}
\centering
\includegraphics[width=\textwidth]{figures/distributed_computing}

** Distributed computing
*** Shared-memory parallelism
   - Typically implemented using threads
   - Processes can communicate through shared memory
   - Low overhead
   - Limited to one computer

*** Distributed parallelism
   - Typically implemented using processes
   - Larger overhead than threads
   - Can usually run on multiple computers

\begin{alertblock}{Note}
  It is not uncommon to see shared-memory parallelism combined, i.e. to have a programm
  running multiple threads in multiple processes distributed across a compute cluster.
\end{alertblock}

** IPython Parallel
*** IPython Parallel (=ipyparallel=) 
    - Distributed-computing package for IPython
    - Engines can run /locally/ or on different computers
      (through e.g. SSH or MPI)
    - Can be used interactively


\vspace{1cm}
\centering
\includegraphics[width=\textwidth]{figures/ipyparallel}

** IPython Parallel
*** Engines
    - A Python process to which you can send code for execution
*** Controller
    - Local process to which engines connect
    - Interface through which communication with engines
      takes place
*** =Client= and =View=
    - Python objects to connect to controller
      and interact with engines.

\vspace{1cm}
\centering
\includegraphics[width=0.7\textwidth]{figures/ipyparallel}

** IPython Parallel
*** Installation
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
    #+BEGIN_SRC bash
    pip install ipyparallel
    #+END_SRC

*** Starting controller and engines
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
    #+BEGIN_SRC bash
    ipcluster start -n 4 # Will start controller and 4 engines locally
    #+END_SRC

*** Connecting to the controller
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    from ipyparallel import Client
    client = Client()
    print(client.ids) # Prints: [0, 1, 2, 3]
    view = client.direct_view()
    #+END_SRC

** IPython Parallel
*** Distributed hello world
    - A view can be used to execute code on the engines.
    - =apply= executes a method on all engines.
    - However, since these engines run in different processes
      no output is produced in the client application.
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    def say_hi():
        import os
        print(f"Hi from process {os.getpid()}")

    results = view.apply(say_hi) # Prints nothing.
    #+END_SRC

** IPython Parallel
*** Distributed hello world
    - However, the returned =AsyncResult= object let's us access
      the output from each process:
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    results.display_outputs()
    #+END_SRC

*** Output
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=light
    #+BEGIN_SRC text
    [stdout:0] Hi from process 10557
    [stdout:1] Hi from process 10569
    [stdout:2] Hi from process 10570
    [stdout:3] Hi from process 10573
    #+END_SRC

** Exercise 3
   - Complete exercise 3 from notebook
   - Time: 10 minutes

** IPython Parallel
*** Blocking and non-blocking execution
    - =apply= executes the given function /asynchonously/, i.e. it returns immediately
      and returns an =AsyncResult= as place holder
    - =apply_sync= is a blocking version of =apply= and returns results immediately

    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    def get_integer():
        return int()

    result = view.apply_sync(get_integer)
    print(result) # Prints: [0, 0, 0, 0]
    #+END_SRC

    - Most other methods accept a =block= keyword arguments which defines
      their behavior.
    - I will use =blocking= behavior in the following example because it makes
      effects directly visible.
    - In general, however, asynchronous behavior is more powerful because it allows
      monitoring the processing state.

** IPython Parallel
*** Complications
    - The client program and the engines don't share state:

    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    import os

    def say_hi():
        print(f"Hi from process {os.getpid()}")

    results = view.apply_async(say_hi)
    #+END_SRC

    #+attr_latex: :options fontsize=\tiny, bgcolor=light
    #+BEGIN_SRC text
    [0:apply]: 
    ---------------------------------------------------------------------------
    NameError                                 Traceback (most recent call last)<string> in <module>
    <ipython-input-49-727b728ca0b2> in say_hi()
    NameError: name 'os' is not defined

    [1:apply]: 
    ---------------------------------------------------------------------------
    NameError                                 Traceback (most recent call last)<string> in <module>
    <ipython-input-49-727b728ca0b2> in say_hi()
    NameError: name 'os' is not defined

    [2:apply]: 
    ---------------------------------------------------------------------------
    NameError                                 Traceback (most recent call last)<string> in <module>
    <ipython-input-49-727b728ca0b2> in say_hi()
    NameError: name 'os' is not defined
    
    ...
    #+END_SRC

** IPython Parallel
*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :END:
**** Normal code
     \vspace{1cm}
     \includegraphics[width=\textwidth]{figures/hotline_bling_normal}

*** A block                                           :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :END:
**** =ipyparallel=
     \vspace{1cm}
     \includegraphics[width=\textwidth]{figures/hotline_bling_ipyparallel}


** Handling imports on engines
1. =sync_imports=:
    - Works only with =DirectView= objects[fn:4]
    - Can't assign aliases for imports
    #+attr_latex: :options fontsize=\footnotesize, bgcolor=dark
    #+BEGIN_SRC Python
    with view.sync_imports():
        import numpy
    #+END_SRC

2. =execute=:
    - Executes code on engines.
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    view.execute('import numpy as np')
    #+END_SRC

3. =require= decorator

    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    @ipp.require('os') # Or ipp.require(os) is os is already imported
    def say_hi():
        print(f"Hi from process {os.getpid()}")
    #+END_SRC

[fn:4] We'll see more details later.
    

** Transferring data to the engines

1. =push= and =pull=
   #+attr_latex: :options fontsize=\footnotesize, bgcolor=dark
   #+BEGIN_SRC Python
   view.push({a: 1, b: 2})
   a = view.pull('a', block=True)
   print(a) # Prints: [1, 1, 1, 1]
   #+END_SRC

2. Dictionary interface
   #+attr_latex: :options fontsize=\footnotesize, bgcolor=dark
   #+BEGIN_SRC Python
   view['a'] = 2
   a = view['a']
   print(a) # Prints: [2, 2, 2, 2]
   #+END_SRC
3. =scatter= and gather:
   - /Distributes/ data across engines:
   #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
   #+BEGIN_SRC Python
   view.scatter('a', [1] * 16)
   sums = view.apply(lambda : sum(a))
   print(sums) # Prints: [4, 4, 4, 4]
   a = view.gather('a', block=True)
   print(a) # Prints: [1, ..., 1]
   #+END_SRC

** Executing code on engines
  1. =execute=:
    - Executes code give as string
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    view.execute('import numpy as np')
    #+END_SRC

  2. =apply=, =apply_async= and =apply_sync=:
    - Executes function on engines
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    def my_function(a, b):
        return a + b
    result = view.apply_sync(my_function, 1, 2)
    print(result) # Prints: [3, 3, 3, 3]
    #+END_SRC

  3. =map=:
    - Maps function to range of arguments across engines:
    #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
    #+BEGIN_SRC Python
    @ipp.require('os')
    def get_pid(dummy_argument):
        return os.getpid()
    result = view.map(get_pid, range(4), block=True)
    print(result) # Prints: [10557, 10569, 10570, 10573]
    #+END_SRC

** Different views
*** Direct view
    - Provides /direct/ access to engines
    - Created from =Client= object using either =direct_view= method
      or list indexing:
      #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
      #+BEGIN_SRC Python
      view = client[::2]
      result = view.map(get_pid, range(4), block=True)
      print(result) # Prints: [10557, 10557, 10570, 10570]
      #+END_SRC

*** Load balanced view
    - Tasks are distributed dynamically in order to balance load
    - Can't target specific engines
      
** Exercise 4
   - Exercise 4 in notebook
   - Time: 5 minutes
** Magic commands
*** =%px=
    - Executes line of code on engines
      
      #+attr_latex: :options fontsize=\scriptsize, bgcolor=dark
      #+BEGIN_SRC Python
      %px import numpy as np
      %px a = np.random.rand(1)
      print(view.scatter('a')) # Prints: [0.05926484 0.23279085 0.74808488 0.80716102]
      #+END_SRC


*** =%%px=
    - This is cell magic, i.e. work only in notebooks
    - Executes all cell content on engines
    - =%%px --block= will display last result from each engine

*** =%autopx=
    - Will execute all subsequent commands on engines until
      next occurrence of =autopx=
      
** Exercise 5
   - Exercise 4 in notebook
   - Time: 15 minutes
